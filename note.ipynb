{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0c55561",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risha\\Desktop\\pypy\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions, AcceleratorDevice, AcceleratorOptions\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from settings import AIConfig\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d89a9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.datamodel.pipeline_options import PictureDescriptionApiOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7df19ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"./data/M2.pdf\"  # file path or URL\n",
    "\n",
    "# source = \"https://arxiv.org/pdf/2408.09869\"\n",
    "picture_desc_api_option = PictureDescriptionApiOptions(\n",
    "    url=\"http://localhost:11434/v1\",\n",
    "    prompt=\"Describe the content of this image in a single paragraph.\",\n",
    "    params=dict(model=\"ollama:ministral-3:8b\", temperature=0.2),\n",
    "    timeout=60\n",
    ")\n",
    "\n",
    "# Configure PdfPipelineOptions for OCR with Tesseract CLI\n",
    "pipeline_options = PdfPipelineOptions(\n",
    "    accelerator_options=AcceleratorOptions(device=AcceleratorDevice.CUDA),\n",
    "    do_picture_description=True,\n",
    "    picture_description_api_option=picture_desc_api_option,\n",
    "    generate_picture_images=True,\n",
    "    enable_remote_services=True,\n",
    "    # do_ocr=False,\n",
    "    images_scale=2,\n",
    "    # ocr_options=TesseractCliOcrOptions(lang=[\"eng\"])\n",
    ")\n",
    "\n",
    "# Initialize DocumentConverter with the configured options\n",
    "converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(\n",
    "            pipeline_options=pipeline_options,\n",
    "            \n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3debed88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "971fd5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-06 19:53:37,205 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-06 19:53:37,610 - INFO - Going to convert document batch...\n",
      "2025-12-06 19:53:37,612 - INFO - Processing document M2.pdf\n",
      "2025-12-06 19:54:47,841 - INFO - Finished converting document M2.pdf in 70.80 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved markdown to ./data/M3.md\n"
     ]
    }
   ],
   "source": [
    "doc = converter.convert(source)\n",
    "markdown = doc.document.export_to_markdown()\n",
    "output_path = \"./data/M3.md\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(markdown)\n",
    "print(f\"Saved markdown to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06d37ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-06 19:56:24,720 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "c:\\Users\\risha\\Desktop\\pypy\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\risha\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "2025-12-06 19:56:30,280 - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.77it/s]\n"
     ]
    }
   ],
   "source": [
    "import lancedb\n",
    "from lancedb.pydantic import LanceModel, Vector\n",
    "from lancedb.embeddings import get_registry\n",
    "from docling.document_converter import DocumentConverter\n",
    "from docling.chunking import HybridChunker\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "MD_FILE_PATH = \"./data/M3.md\"  # Your converted markdown file\n",
    "DB_URI = \"./lancedb_data\"    # This will create a local folder\n",
    "TABLE_NAME = \"engineering_docs\"\n",
    "\n",
    "embedding_func = get_registry().get(\"sentence-transformers\").create()\n",
    "\n",
    "class DocChunk(LanceModel):\n",
    "    \"\"\"\n",
    "    Pydantic schema for LanceDB.\n",
    "    \"\"\"\n",
    "    text: str = embedding_func.SourceField() # The text to be embedded\n",
    "    vector: Vector(embedding_func.ndims()) = embedding_func.VectorField() # type: ignore # Auto-generated\n",
    "    \n",
    "    # Metadata fields\n",
    "    filename: str\n",
    "    chunk_index: int\n",
    "    chunk_type: str  # e.g., \"text\", \"table\", \"code\"\n",
    "\n",
    "db = lancedb.connect(\"./lancedb_data\")\n",
    "table_name = \"engineering_notes\"\n",
    "\n",
    "table = db.create_table(table_name, schema=DocChunk, mode=\"overwrite\")\n",
    "def process_and_store_md(file_path: str):\n",
    "    print(f\"Processing: {file_path}\")\n",
    "    converter = DocumentConverter()\n",
    "    result = converter.convert(file_path)\n",
    "    doc = result.document\n",
    "    chunker = HybridChunker(\n",
    "        max_tokens=512,\n",
    "        merge_peers=True,\n",
    "    )\n",
    "\n",
    "    chunk_iter = chunker.chunk(doc)\n",
    "    data_to_ingest = []\n",
    "    for i, chunk in enumerate(chunk_iter):\n",
    "        headers = [h for h in chunk.meta.headings]\n",
    "        hierarchy_path = \" > \".join(headers) if headers else \"Root\"\n",
    "        content_type = \"mixed\"\n",
    "        if \"```\" in chunk.text:\n",
    "            content_type = \"code\"\n",
    "        elif \"|\" in chunk.text and \"-|-\" in chunk.text:\n",
    "            content_type = \"table\"\n",
    "\n",
    "        entry = {\n",
    "            \"text\": chunk.text,\n",
    "            \"filename\": os.path.basename(\"M2.md\"),\n",
    "            \"chunk_type\": content_type,\n",
    "            \"chunk_index\": i,\n",
    "        }\n",
    "        data_to_ingest.append(entry)\n",
    "    if data_to_ingest:\n",
    "        table.add(data_to_ingest)\n",
    "        print(f\"Successfully added {len(data_to_ingest)} chunks to LanceDB.\")\n",
    "    else:\n",
    "        print(\"No chunks generated.\")\n",
    "    table.create_fts_index(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7efb1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-06 19:56:59,629 - INFO - detected formats: [<InputFormat.MD: 'md'>]\n",
      "2025-12-06 19:56:59,631 - INFO - Going to convert document batch...\n",
      "2025-12-06 19:56:59,632 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2025-12-06 19:56:59,632 - INFO - Processing document M3.md\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ./data/M3.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-06 19:57:06,388 - INFO - Finished converting document M3.md in 6.75 sec.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (548 > 512). Running this sequence through the model will result in indexing errors\n",
      "Batches: 100%|██████████| 3/3 [00:02<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added 91 chunks to LanceDB.\n"
     ]
    }
   ],
   "source": [
    "process_and_store_md(MD_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a2a4b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.61it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    The main issue is complexity at the sender and...\n",
       "1    1-persistent CSMA - When a station has data to...\n",
       "2    This technique allows data frames to contain a...\n",
       "3    In Go back N, sender window size is N and rece...\n",
       "4    A method for doubling the capacity of an ALOHA...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = table.search(\"Explain selective repeat\").distance_type(\"cosine\").limit(5)\n",
    "res.to_pandas()['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e06d5829",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 132.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid search results:\n",
      "                                                text  \\\n",
      "0  The physical layer corresponds to the OSI phys...   \n",
      "1  - physical radio layer, deals with radio trans...   \n",
      "2  1. A framing method - that shows the end of on...   \n",
      "3  PPP -Point to Point Protocol\\nHDLC - High leve...   \n",
      "4  Data link control handles framing, flow contro...   \n",
      "5  Networks can be divided into two categories:\\n...   \n",
      "6  The data link layer uses the services of the p...   \n",
      "7  - Start frame delimiter (SFD)-Alerts each stat...   \n",
      "8  The data link layer have to detect and, if nec...   \n",
      "9  The principal service is transferring data fro...   \n",
      "\n",
      "                                              vector filename  chunk_index  \\\n",
      "0  [-0.04163005, -0.0043371688, 0.009071809, 0.02...    M2.md           74   \n",
      "1  [-0.07774799, -0.041657556, -0.07229016, 0.038...    M2.md           86   \n",
      "2  [-0.04260041, 0.031152837, -0.042637832, -0.05...    M2.md           28   \n",
      "3  [-0.058674913, -0.05213298, -0.08557983, -0.00...    M2.md           20   \n",
      "4  [-0.03276039, -0.02095756, -0.06589061, -0.029...    M2.md           34   \n",
      "5  [0.044851217, -0.0926191, -0.028364519, -0.043...    M2.md           33   \n",
      "6  [-0.0610165, 0.016010715, 0.063476175, 0.02412...    M2.md            0   \n",
      "7  [0.022083934, -0.021794016, -0.12758248, -0.03...    M2.md           56   \n",
      "8  [0.001267123, 0.0066179074, 0.0059557413, 0.01...    M2.md            3   \n",
      "9  [-0.13847253, -0.037086964, -0.025512973, -0.0...    M2.md            2   \n",
      "\n",
      "  chunk_type  _relevance_score  \n",
      "0      mixed          0.031514  \n",
      "1      mixed          0.031498  \n",
      "2      mixed          0.031099  \n",
      "3      mixed          0.030622  \n",
      "4      mixed          0.030331  \n",
      "5      mixed          0.016393  \n",
      "6      mixed          0.015873  \n",
      "7      mixed          0.015385  \n",
      "8      mixed          0.015152  \n",
      "9      mixed          0.015152  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from lancedb.rerankers import RRFReranker\n",
    "reranker = RRFReranker()\n",
    "results = (\n",
    "    table.search(\n",
    "        \"Explain Logical Link Control\",\n",
    "        query_type=\"hybrid\",\n",
    "        vector_column_name=\"vector\",\n",
    "        fts_columns=\"text\",\n",
    "    )\n",
    "    .rerank(reranker)\n",
    "    .limit(10)\n",
    "    .to_pandas()\n",
    ")\n",
    "\n",
    "print(\"Hybrid search results:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68607a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pypy (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
